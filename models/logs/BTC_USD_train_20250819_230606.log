INFO:__main__:Logging to models\logs\BTC_USD_train_20250819_230609.log
INFO:__main__:No config file provided or found, using defaults.
INFO:__main__:Loading data for BTC_USD from data
INFO:agents.asset_universe:Loaded filtered data for BTC_USD: 16529 rows
INFO:__main__:Data shape: (16529, 5)
INFO:__main__:Data columns: ['close', 'high', 'low', 'open', 'volume']
INFO:__main__:Data sample: 
                                  close          high  ...          open  volume
datetime                                               ...                      
2023-06-16 20:00:00+00:00  26394.544922  26436.359375  ...  26361.851562     0.0
2023-06-16 21:00:00+00:00  26282.353516  26395.740234  ...  26394.169922     0.0
2023-06-16 22:00:00+00:00  26355.738281  26370.039062  ...  26282.064453     0.0

[3 rows x 5 columns]
INFO:agents.ppo_agent:Mixed precision training enabled for faster training on RTX 3090
D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\vec_env\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.
  warnings.warn(
D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
INFO:__main__:Starting training for BTC_USD for 100000 timesteps
INFO:agents.ppo_agent:Starting training of BTC_USD agent for 100000 timesteps
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to models\logs\BTC_USD\BTC_USD_8
----------------------------------
| time/              |           |
|    fps             | 95        |
|    iterations      | 1         |
|    time_elapsed    | 21        |
|    total_timesteps | 2048      |
| trading/           |           |
|    daily_return    | -0.000999 |
|    drawdown        | -0.526    |
|    max_drawdown    | -0.529    |
|    portfolio_value | 5.09e+03  |
|    sharpe_ratio    | -1.77     |
----------------------------------
Traceback (most recent call last):
  File "D:\Work\ai_trading_system\agents\train_single_agent.py", line 117, in <module>
    main() 
    ^^^^^^
  File "D:\Work\ai_trading_system\agents\train_single_agent.py", line 113, in main
    agent.train(total_timesteps=args.timesteps)
  File "D:\Work\ai_trading_system\agents\ppo_agent.py", line 171, in train
    self.model.learn(
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\ppo\ppo.py", line 311, in learn
    return super().learn(
           ^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 202, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
                                 ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\policies.py", line 647, in forward
    latent_pi, latent_vf = self.mlp_extractor(features)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\torch_layers.py", line 257, in forward
    return self.forward_actor(features), self.forward_critic(features)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\torch_layers.py", line 260, in forward_actor
    return self.policy_net(features)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\torch\nn\modules\container.py", line 240, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\torch\nn\modules\module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\torch\nn\modules\linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
