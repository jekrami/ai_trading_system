INFO:__main__:Logging to models\logs\BTC_USD_train_20250520_142958.log
INFO:__main__:No config file provided or found, using defaults.
INFO:__main__:Loading data for BTC_USD from data
INFO:agents.asset_universe:Loaded filtered data for BTC_USD: 16529 rows
INFO:__main__:Data shape: (16529, 5)
INFO:__main__:Data columns: ['close', 'high', 'low', 'open', 'volume']
INFO:__main__:Data sample: 
                                  close          high  ...          open  volume
datetime                                               ...                      
2023-06-16 20:00:00+00:00  26394.544922  26436.359375  ...  26361.851562     0.0
2023-06-16 21:00:00+00:00  26282.353516  26395.740234  ...  26394.169922     0.0
2023-06-16 22:00:00+00:00  26355.738281  26370.039062  ...  26282.064453     0.0

[3 rows x 5 columns]
D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\vec_env\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.
  warnings.warn(
D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
INFO:__main__:Starting training for BTC_USD for 100000 timesteps
INFO:agents.ppo_agent:Starting training of BTC_USD agent for 100000 timesteps
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to models\logs\BTC_USD\BTC_USD_5
---------------------------------
| time/              |          |
|    fps             | 91       |
|    iterations      | 1        |
|    time_elapsed    | 22       |
|    total_timesteps | 2048     |
| trading/           |          |
|    daily_return    | 0        |
|    drawdown        | -0.487   |
|    max_drawdown    | -0.489   |
|    portfolio_value | 5.49e+03 |
|    sharpe_ratio    | -1.59    |
---------------------------------
Traceback (most recent call last):
  File "D:\Work\ai_trading_system\agents\train_single_agent.py", line 117, in <module>
    main() 
    ^^^^^^
  File "D:\Work\ai_trading_system\agents\train_single_agent.py", line 113, in main
    agent.train(total_timesteps=args.timesteps)
  File "D:\Work\ai_trading_system\agents\ppo_agent.py", line 161, in train
    self.model.learn(
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\ppo\ppo.py", line 311, in learn
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\vec_env\base_vec_env.py", line 222, in step
    return self.step_wait()
           ^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\vec_env\dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
                                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\monitor.py", line 94, in step
    observation, reward, terminated, truncated, info = self.env.step(action)
                                                       ^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\shimmy\openai_gym_compatibility.py", line 250, in step
    obs, reward, done, info = self.gym_env.step(action)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\envs\trading_env.py", line 174, in step
    observation = self._get_observation()
                  ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\envs\trading_env.py", line 236, in _get_observation
    features = self._calculate_indicators()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\envs\trading_env.py", line 89, in _calculate_indicators
    df['volume_change'] = df['volume'].pct_change().fillna(0)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\pandas\core\generic.py", line 12136, in pct_change
    mask = col.isna().values
           ^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\pandas\core\series.py", line 5775, in isna
    return NDFrame.isna(self)
           ^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\pandas\core\generic.py", line 8754, in isna
    return isna(self).__finalize__(self, method="isna")
           ^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\pandas\core\dtypes\missing.py", line 178, in isna
    return _isna(obj)
           ^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\pandas\core\dtypes\missing.py", line 216, in _isna
    result = _isna_array(obj._values, inf_as_na=inf_as_na)
                         ^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\pandas\core\series.py", line 863, in _values
    return self._mgr.internal_values()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\pandas\core\internals\managers.py", line 2006, in internal_values
    return self._block.values
           ^^^^^^^^^^^
  File "properties.pyx", line 36, in pandas._libs.properties.CachedProperty.__get__
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\pandas\core\internals\managers.py", line 1940, in _block
    @cache_readonly

KeyboardInterrupt
