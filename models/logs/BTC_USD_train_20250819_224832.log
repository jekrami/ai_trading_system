INFO:__main__:Logging to models\logs\BTC_USD_train_20250819_224835.log
INFO:__main__:No config file provided or found, using defaults.
INFO:__main__:Loading data for BTC_USD from data
INFO:agents.asset_universe:Loaded filtered data for BTC_USD: 16529 rows
INFO:__main__:Data shape: (16529, 5)
INFO:__main__:Data columns: ['close', 'high', 'low', 'open', 'volume']
INFO:__main__:Data sample: 
                                  close          high  ...          open  volume
datetime                                               ...                      
2023-06-16 20:00:00+00:00  26394.544922  26436.359375  ...  26361.851562     0.0
2023-06-16 21:00:00+00:00  26282.353516  26395.740234  ...  26394.169922     0.0
2023-06-16 22:00:00+00:00  26355.738281  26370.039062  ...  26282.064453     0.0

[3 rows x 5 columns]
D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\vec_env\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.
  warnings.warn(
D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.
  warnings.warn(
INFO:__main__:Starting training for BTC_USD for 100000 timesteps
INFO:agents.ppo_agent:Starting training of BTC_USD agent for 100000 timesteps
Using cuda device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to models\logs\BTC_USD\BTC_USD_7
---------------------------------
| time/              |          |
|    fps             | 86       |
|    iterations      | 1        |
|    time_elapsed    | 23       |
|    total_timesteps | 2048     |
| trading/           |          |
|    daily_return    | 0        |
|    drawdown        | -0.547   |
|    max_drawdown    | -0.548   |
|    portfolio_value | 4.65e+03 |
|    sharpe_ratio    | -2.31    |
---------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 2           |
|    time_elapsed         | 49          |
|    total_timesteps      | 4096        |
| trading/                |             |
|    daily_return         | 0           |
|    drawdown             | -0.749      |
|    max_drawdown         | -0.75       |
|    portfolio_value      | 2.57e+03    |
|    sharpe_ratio         | -2.03       |
| train/                  |             |
|    approx_kl            | 0.011928653 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.09       |
|    explained_variance   | -10.1       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0305      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.437       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 3           |
|    time_elapsed         | 75          |
|    total_timesteps      | 6144        |
| trading/                |             |
|    daily_return         | -0.00056    |
|    drawdown             | -0.82       |
|    max_drawdown         | -0.838      |
|    portfolio_value      | 1.85e+03    |
|    sharpe_ratio         | -1.43       |
| train/                  |             |
|    approx_kl            | 0.012751685 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.07       |
|    explained_variance   | -6.81       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0146     |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.0589      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 4           |
|    time_elapsed         | 99          |
|    total_timesteps      | 8192        |
| trading/                |             |
|    daily_return         | 0           |
|    drawdown             | -0.896      |
|    max_drawdown         | -0.897      |
|    portfolio_value      | 1.06e+03    |
|    sharpe_ratio         | -1.29       |
| train/                  |             |
|    approx_kl            | 0.014781104 |
|    clip_fraction        | 0.187       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.05       |
|    explained_variance   | -4.31       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0514     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.03       |
|    value_loss           | 0.0234      |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 5           |
|    time_elapsed         | 123         |
|    total_timesteps      | 10240       |
| trading/                |             |
|    daily_return         | 0.00224     |
|    drawdown             | -0.945      |
|    max_drawdown         | -0.946      |
|    portfolio_value      | 565         |
|    sharpe_ratio         | -1.25       |
| train/                  |             |
|    approx_kl            | 0.015739774 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.04       |
|    explained_variance   | -11.7       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0428     |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0346     |
|    value_loss           | 0.00789     |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 6           |
|    time_elapsed         | 148         |
|    total_timesteps      | 12288       |
| trading/                |             |
|    daily_return         | -0.00808    |
|    drawdown             | -0.968      |
|    max_drawdown         | -0.97       |
|    portfolio_value      | 332         |
|    sharpe_ratio         | -1.22       |
| train/                  |             |
|    approx_kl            | 0.015971612 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.03       |
|    explained_variance   | -7.75       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0764     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.00273     |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 82          |
|    iterations           | 7           |
|    time_elapsed         | 173         |
|    total_timesteps      | 14336       |
| trading/                |             |
|    daily_return         | -0.000611   |
|    drawdown             | -0.984      |
|    max_drawdown         | -0.984      |
|    portfolio_value      | 166         |
|    sharpe_ratio         | -1.23       |
| train/                  |             |
|    approx_kl            | 0.017079625 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | -6.02       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0643     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 0.00152     |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 81          |
|    iterations           | 8           |
|    time_elapsed         | 199         |
|    total_timesteps      | 16384       |
| trading/                |             |
|    daily_return         | 0           |
|    drawdown             | -0.993      |
|    max_drawdown         | -0.993      |
|    portfolio_value      | 76.4        |
|    sharpe_ratio         | -1.24       |
| train/                  |             |
|    approx_kl            | 0.018892046 |
|    clip_fraction        | 0.216       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.01       |
|    explained_variance   | -7.87       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0487     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0301     |
|    value_loss           | 0.00205     |
-----------------------------------------
Traceback (most recent call last):
  File "D:\Work\ai_trading_system\agents\train_single_agent.py", line 117, in <module>
    main() 
    ^^^^^^
  File "D:\Work\ai_trading_system\agents\train_single_agent.py", line 113, in main
    agent.train(total_timesteps=args.timesteps)
  File "D:\Work\ai_trading_system\agents\ppo_agent.py", line 161, in train
    self.model.learn(
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\ppo\ppo.py", line 311, in learn
    return super().learn(
           ^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 224, in collect_rollouts
    if not callback.on_step():
           ^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
           ^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\callbacks.py", line 223, in _on_step
    continue_training = callback.on_step() and continue_training
                        ^^^^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\.venv\Lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
           ^^^^^^^^^^^^^^^
  File "D:\Work\ai_trading_system\agents\ppo_agent.py", line 48, in _on_step
    peak = max(self.portfolio_values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
